{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "sys.path.append(\"{}/dev/github/ml-toolkit\".format(home_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.utils.data.text.vectorizer import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(default_indexes={0: '<pad>'})\n",
    "vectorizer.load_dictionary('/home/vdw/data/datasets/hotel-reviews-txt/dictionary', word_col=0)\n",
    "\n",
    "print(vectorizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_list = []\n",
    "with open('{}/data/datasets/hotel-reviews-txt/train_permute.txt'.format(home_dir)) as infile:\n",
    "    for idx, line in enumerate(infile):\n",
    "        seq = [ int(i) for i in line.strip().split()]\n",
    "        train_seq_list.append(seq)\n",
    "\n",
    "X, indices = vectorizer.prepare_sequences(train_seq_list, auto_padding=True, max_len=100, unknown_idx=1)\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "# Free up some memory\n",
    "train_seq_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get max index (parameter for embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = int(np.max([ np.max(seq) for seq in X ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[1])\n",
    "print(vectorizer.vocabulary.get_words(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "X_train = X[:num_samples]\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.utils.data.text.wordvectorloader import WordVectorLoader\n",
    "\n",
    "use_pretrained_embeddings = False\n",
    "\n",
    "if use_pretrained_embeddings is True:\n",
    "    word_vector_loader = WordVectorLoader(300)\n",
    "    embed_mat = word_vector_loader.create_embedding_matrix('{}/data/dumps/glove/glove.840B.300d.txt'.format(home_dir), vectorizer.vocabulary.word_to_index, max_idx, init='random', verbatim=True)\n",
    "    print(embed_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.utils.data.text.dataset import BucketBatchSampler, BucketDataset\n",
    "\n",
    "bucket_batch_sampler = BucketBatchSampler(X_train, batch_size)\n",
    "bucket_dataset = BucketDataset(X_train, None)\n",
    "\n",
    "X_train_iter = DataLoader(bucket_dataset, batch_size=1, batch_sampler=bucket_batch_sampler, shuffle=False, num_workers=8, drop_last=False)\n",
    "\n",
    "print(len(X_train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.models.text.autoencoder.textcnnae import Parameters, ConvMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{}/data/ml-toolkit/pytorch-models/text-cnn-ae/'.format(home_dir)\n",
    "\n",
    "params = {'conv_mode': ConvMode.D1,\n",
    "          'max_seq_len': 100,\n",
    "          'vocab_size': max_idx+1,\n",
    "          'embed_dim': 300,\n",
    "          'encoder_lr': 0.0001,\n",
    "          'decoder_lr': 0.0001,\n",
    "          'kernel_sizes': [3, 3, -1],\n",
    "          'strides': [2, 2, 2],\n",
    "          'num_filters': [300, 600, 500],\n",
    "          'output_paddings': [1, 0, 0],\n",
    "          'do_batch_norm': True,\n",
    "          'dropout_ratio': 0.5,\n",
    "          'tau': 0.01,\n",
    "          'clip': 0.25\n",
    "          }\n",
    "\n",
    "\n",
    "print(params)\n",
    "with open(path+'params.json', 'w') as outfile:\n",
    "    json.dump(params, outfile)\n",
    "\n",
    "params = Parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.models.text.autoencoder.textcnnae import TextCnnAE\n",
    "criterion = nn.NLLLoss()\n",
    "text_cnn_ae = TextCnnAE(device, params, criterion)\n",
    "\n",
    "print(text_cnn_ae.encoder)\n",
    "print(text_cnn_ae.decoder)\n",
    "print(text_cnn_ae.params.kernel_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pretrained word embeddings if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentence_vae.embedding.weight.data[0])\n",
    "if use_pretrained_embeddings is True:\n",
    "    print(\"Initialize embdding layer with pretrained embeddings\")\n",
    "    text_cnn_ae.embedding.weight.data.copy_(torch.from_numpy(embed_mat))\n",
    "    text_cnn_ae.embedding.weight.data = F.normalize(text_cnn_ae.embedding.weight.data, p=2, dim=1)\n",
    "    text_cnn_ae.embedding.weight.requires_grad = False\n",
    "else:\n",
    "    text_cnn_ae.embedding.weight.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "safe_after_epoch = False\n",
    "\n",
    "encoder_file_name = '{}/data/ml-toolkit/pytorch-models/text-cnn-ae/textcnnae-encoder.model'.format(home_dir)\n",
    "decoder_file_name = '{}/data/ml-toolkit/pytorch-models/text-cnn-ae/textcnnae-decoder.model'.format(home_dir)\n",
    "\n",
    "text_cnn_ae.train()\n",
    "\n",
    "text_cnn_ae.set_learning_rates(0.001, 0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = text_cnn_ae.train_epoch(epoch, X_train_iter, verbatim=True)\n",
    "    print(epoch_loss)\n",
    "    losses.append(epoch_loss)\n",
    "    if safe_after_epoch:\n",
    "        text_cnn_ae.save_models(encoder_file_name, decoder_file_name)\n",
    "    text_cnn_ae.update_learning_rates(0.99, 0.99)        \n",
    "        \n",
    "text_cnn_ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loss = np.max(losses)\n",
    "losses_normalized = losses / max_loss\n",
    "\n",
    "plt.plot(losses_normalized, label='loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('CNN-AE (e_dim={})'.format(params.embed_dim))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequence(sequence, model, vectorizer):\n",
    "    original_sequence = vectorizer.sequence_to_text(sequence)\n",
    "    #print(original_sequence)\n",
    "    X = torch.tensor([sequence], dtype=torch.long).to(model.device)\n",
    "    #print(X)\n",
    "    decoded_indices = model.generate(X)\n",
    "    decoded_sequence = vectorizer.sequence_to_text(decoded_indices)\n",
    "    return ' '.join(original_sequence), ' '.join(decoded_sequence)\n",
    "    \n",
    "#print(X[0])\n",
    "print(check_sequence(X[0] ,text_cnn_ae, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a sample of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, s in enumerate(X):\n",
    "    original, decoded = check_sequence(s, text_cnn_ae, vectorizer)\n",
    "    print(\"================================================\")\n",
    "    print()\n",
    "    print(original)\n",
    "    print(\">>>\")\n",
    "    print(decoded)\n",
    "    print()\n",
    "    if idx > 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_list = []\n",
    "with open('{}/data/datasets/hotel-reviews-txt/test.txt'.format(home_dir)) as infile:\n",
    "    for idx, line in enumerate(infile):\n",
    "        seq = [ int(i) for i in line.strip().split()]\n",
    "        test_seq_list.append(seq)\n",
    "\n",
    "X_test, _ = vectorizer.prepare_sequences(test_seq_list, auto_padding=True, max_len=100, unknown_idx=1)\n",
    "\n",
    "print(X_test[0])\n",
    "\n",
    "test_seq_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, s in enumerate(X_test):\n",
    "    original, decoded = check_sequence(s, text_cnn_ae, vectorizer)\n",
    "    print(\"================================================\\n\")\n",
    "    print(' '.join([t for t in original.split() if t != '<pad>' ]))\n",
    "    print(\">>>\")\n",
    "    print(' '.join([t for t in decoded.split() if t != '<pad>' ]))\n",
    "    print(\"\\n\")\n",
    "    if idx > 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
